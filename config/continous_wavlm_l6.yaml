#
# TRAIN the blind separation using the recipe provided in DASB
#

### seed ###
sampler_seed: 1234
seed: 1234

### ddp config ###
gpus: [0,1,2,3]
port: 12355

### train ###
trainer: !name:exp.tsslm.discrete_k_1000.trainer.Trainer
epoch: 100
find_unused: False
pre_eval: True
best_field: error
best_save_type: descend #[descend, ascend] ## descend means that the the lower the value the best_field, the better it will be
max_ckpt: 1

### optim and scheduler ###
lr: 0.0005
optim: !name:torch.optim.AdamW
  lr: !ref <lr>
  betas: (0.9, 0.98)
  eps: 1.e-8
  weight_decay: 0.01
new_bob: !new:scheduler.schedulers.NewBobScheduler
  initial_value: !ref <lr>
  annealing_factor: 0.9
  improvement_threshold: 0.0025
  patient: 1

### model ###
num_clusters: 1000
embedding_dim: 1024
d_model: 512
num_heads: 8
num_layers: 8
ssl_layers: [1, 3, 7, 12, 18, 23]

FiLM: !new:models.modules.film.FiLM
    size: !ref <embedding_dim>

cross_attention_model: !new:models.modules.transformer_encoder_cross.TransformerEncoderCross
    num_layers: 4
    d_model: !ref <embedding_dim>
    nhead: 16
    d_ffn: 1024
    dropout: 0
fusion_norm: !new:torch.nn.GroupNorm
  num_groups: 1
  num_channels: !ref <embedding_dim>
  eps: 1.e-8

cache_ckpt_path: /public/home/qinxy/bltang/ml_framework_slurm/ckpt # checkpoint, does not matter too much here.
hifi_gan_source: /public/home/qinxy/bltang/pretrain_dasb_tokenizer/models/hifigan-wavlm-l1-3-7-12-18-23-k1000-LibriTTS

hifi_gan: !apply:speechbrain.inference.vocoders.UnitHIFIGAN.from_hparams
  source: !ref <hifi_gan_source>

ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM
  source: /public/home/qinxy/bltang/pretrain_dasb_tokenizer/models/wavlm-large
  save_path: ./ckpt/.wavlm
  output_all_hiddens: True 
  output_norm: False
  freeze: True 
  freeze_feature_extractor: True

discrete_ssl: !new:speechbrain.lobes.models.huggingface_transformers.discrete_ssl.DiscreteSSL
  save_path: ./ckpt/.discrete
  ssl_model: !ref <ssl_model>
  kmeans_dataset: LibriSpeech-100-360-500
  kmeans_repo_id: /public/home/qinxy/bltang/pretrain_dasb_tokenizer/models/SSL_Quantization
  num_clusters: !ref <num_clusters>

attention_mlp: !new:exp.dasb.blind.wavlm.modules.AttentionMLP
  input_dim: !ref <embedding_dim>
  hidden_dim: !ref <embedding_dim>

embedding: !new:exp.dasb.blind.wavlm.modules.Discrete_EmbeddingLayer
  num_codebooks: !apply:exp.dasb.blind.wavlm.utils.get_len 
    array: !ref <ssl_layers>
  vocab_size: !ref <num_clusters>
  emb_dim: !ref <embedding_dim>
  freeze: False

head: !new:torch.nn.Linear 
  in_features: !ref <d_model>
  out_features: !apply:exp.dasb.blind.wavlm.utils.len_ 
    [1, !ref <ssl_layers>, !ref <num_clusters>]

lm_model: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR
    input_size: !ref <embedding_dim>
    tgt_vocab: -1
    d_model: !ref <d_model>
    nhead: !ref <num_heads>
    num_encoder_layers: !ref <num_layers>
    num_decoder_layers: 0
    d_ffn: 2048
    dropout: 0.1
    activation: !name:torch.nn.GELU
    max_length: 2000
    encoder_module: conformer
    normalize_before: True
    causal: False

model: !new:exp.tsslm.discrete_k_1000.model.Model
  hifi_gan: !ref <hifi_gan>
  discrete_ssl: !ref <discrete_ssl>
  ssl_layers: !ref <ssl_layers>
  attention_mlp: !ref <attention_mlp>
  lm_model: !ref <lm_model>
  embedding: !ref <embedding>
  head: !ref <head>
  base_loss: !name:exp.dasb.blind.wavlm.loss.base_loss_ce
  vocab_size: !ref <num_clusters>
  fusion: !ref <cross_attention_model>
  film: !ref <FiLM>
  fusion_norm: !ref <fusion_norm>
  share_emb: False


### data ###
tr_dataset: !name:data.target_dataset.TargetDMDataset ## libri speech DM dataset
  scp_path: /public/home/qinxy/zbang/data/LibriSpeech/scp/train/train_clean_100_360.pt
  epoch_num: 5_0000
  mix_length: 48080
  regi_length: 64080
cv_dataset: !name:data.target_dataset.TargetDataset
  mix_path: "/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/wav16k/min/lists/test/mix.scp"
  regi_path: "/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/wav16k/min/lists/test/bltang/regi.scp"
  clean_path: "/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/wav16k/min/lists/test/s1.scp"
  mix_length: 48080
  regi_length: 64080
batch_size: 128
num_workers: 2
batch_size_eval: 256

### log ###
log_interval: 5